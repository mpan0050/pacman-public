================================================================================
FIT5222 ASSIGNMENT 2 - PACMAN CAPTURE THE FLAG
COMPREHENSIVE IMPLEMENTATION LOG
================================================================================
Student: Mohit Pandya
Student ID: 35295252
Date Started: November 1, 2025
Last Updated: November 7, 2025
Assignment Due: November 3, 2025 (11:55 PM) - SUBMITTED
Contest Status: NOMINATED (Branch: a24b2ec)
Current Performance: 1081 EP / 10W 12L 0T (45.5% win rate)

================================================================================
OVERVIEW OF IMPLEMENTATION
================================================================================

This document provides a comprehensive record of the PDDL-based high-level 
planning implementation for the Pacman Capture the Flag assignment.

FINAL SUBMISSION STATUS:
✓ Submitted: November 3, 2025 (before deadline)
✓ Contest Entry: Nominated on server (commit a24b2ec)
✓ Current Record: 10 Wins, 12 Losses, 0 Ties (1081 EP)
✓ Win Rate: 45.5% (below target of 57%)

KEY IMPLEMENTATION APPROACH:
- Hybrid architecture: PDDL for strategic planning + greedy heuristics for execution
- 8 PDDL actions (attack, attack_aggressive, hunt_invader, defence, get_capsule, 
  hunt_scared, return_food, go_home, patrol)
- Simple 3-tier goal prioritization system
- Reactive low-level movement with ghost avoidance

IMPLEMENTATION CHALLENGES ENCOUNTERED:
1. PDDL planning overhead causing timeout issues in time-constrained mode
2. Simple goal system not adapting well to dynamic game situations
3. Low-level heuristics sometimes conflicting with high-level PDDL plans
4. Agent coordination limited - no information sharing between agents
5. Ghost avoidance sometimes too conservative, missing scoring opportunities

TARGET ACHIEVEMENTS:
✓ Criteria 1 (Baseline): Aimed for >57% win rate - PARTIALLY ACHIEVED (45.5%)
✓ Criteria 2 (Strategy): Aimed for DISTINCTION level - CREDIT level achieved
  - NEW PDDL actions: ✓ Implemented (8 actions)
  - New goal functions: ✓ Limited (basic 3-tier system)
  - Dynamic goal switching: ✗ Not implemented (static priority)
  - Improved low-level planner: ✗ Basic heuristics only

================================================================================
PART 1: PDDL DOMAIN IMPLEMENTATION (myTeam.pddl)
================================================================================

OVERVIEW:
The PDDL domain defines 8 strategic actions for Pacman agents to execute
during the Capture the Flag game. The domain uses STRIPS planning with typing.

TYPE HIERARCHY:
- object (base type)
  ├── enemy (opponent agents)
  │   ├── enemy1 (first opponent)
  │   └── enemy2 (second opponent)
  └── team (our agents)
      ├── ally (teammate)
      └── current_agent (agent making decision)

PREDICATES IMPLEMENTED:
Basic Predicates (core game state):
- (enemy_around ?e ?a): Enemy within 4 grid distance
- (is_pacman ?x): Agent is in enemy territory
- (food_in_backpack ?a): Agent carrying food
- (food_available): Food still exists on enemy territory

Virtual Predicates (goal states):
- (defend_foods): Virtual state for defensive patrol goal

Advanced Predicates (detailed state tracking):
Distance predicates:
- (enemy_long_distance ?e ?a): Enemy >25 units away
- (enemy_medium_distance ?e ?a): Enemy >15 units away  
- (enemy_short_distance ?e ?a): Enemy <15 units away

Food quantity predicates:
- (2_food_in_backpack ?a): Carrying 2+ food
- (3_food_in_backpack ?a): Carrying 3+ food
- (5_food_in_backpack ?a): Carrying 5+ food
- (10_food_in_backpack ?a): Carrying 10+ food
- (20_food_in_backpack ?a): Carrying 20+ food

Proximity predicates:
- (near_food ?a): Food within 4 grid distance
- (near_capsule ?a): Capsule within 4 grid distance

Tactical predicates:
- (capsule_available): Power capsule exists on map
- (is_scared ?x): Agent/ghost has scared timer active
- (can_hunt_ghosts): Scared ghosts available to hunt
- (scared_ghost_near ?a): Scared ghost within 4 grid distance

Score predicates:
- (winning): Team score > enemy score
- (winning_gt3): Leading by 3+ points
- (winning_gt5): Leading by 5+ points
- (winning_gt10): Leading by 10+ points
- (winning_gt20): Leading by 20+ points

Role predicates:
- (invaders_present): At least one enemy in our territory
- (is_attacker ?a): Agent assigned attacker role
- (is_defender ?a): Agent assigned defender role

Cooperative predicates (not currently used):
- (eat_enemy ?a): Ally is chasing enemy
- (go_home ?a): Ally is returning home
- (go_enemy_land ?a): Ally is attacking
- (eat_capsule ?a): Ally is getting capsule
- (eat_food ?a): Ally is collecting food

IMPORTANT NOTE: 
The documentation originally mentioned 13 actions, but the actual implemented
PDDL file contains 8 actions. The additional actions described were planned
but not included in the final submission.

================================================================================
IMPLEMENTATION EVOLUTION - STEP-BY-STEP PROGRESSION
================================================================================

This section documents the chronological development of the Pacman AI agent,
showing how the implementation evolved through multiple iterations.

--------------------------------------------------------------------------------
ITERATION 1: Pure PDDL with Q-Learning (myTeam_old_backup.py)
--------------------------------------------------------------------------------
File: Assignment_2/backup_files/myTeam_old_backup.py
Lines of Code: 708
Date: Early November 2025
Agent Type: MixedAgent

APPROACH:
- Full PDDL integration with Q-learning for low-level planning
- Complex agent with both high-level (PDDL) and low-level (Q-learning) planners
- Training mode with weight persistence

KEY FEATURES:
✓ PDDL solver for high-level action selection
✓ Q-learning with multiple weight sets (offensive, defensive, escape)
✓ Low-level planners for different scenarios
✓ Training mode with epsilon-greedy exploration
✓ Weight persistence to file (QLWeightsMyTeam.txt)
✓ Shared class variables for agent coordination

PDDL ACTIONS USED:
- attack
- attack_aggressive
- hunt_invader
- defence
- return_food
- get_capsule
- hunt_scared
- go_home
- patrol

Q-LEARNING WEIGHT SETS:
1. offensiveWeights: For food collection
   - closest-food: -3
   - bias: 1
   - #-of-ghosts-1-step-away: -100
   - successorScore: 100
   - chance-return-food: 10

2. defensiveWeights: For defending territory
   - numInvaders: -1000
   - onDefense: 100
   - invaderDistance: -10
   - stop: -100
   - reverse: -2

3. escapeWeights: For returning home safely
   - onDefense: 1000
   - enemyDistance: 30
   - stop: -100
   - distanceToHome: -20

PROBLEMS ENCOUNTERED:
✗ High computational complexity (PDDL + Q-learning)
✗ Timeout issues in time-constrained mode (-c flag)
✗ Training required extensive game runs
✗ PDDL planning sometimes conflicted with Q-learning decisions
✗ Difficult to debug and tune both systems simultaneously

PERFORMANCE:
- Not formally tested due to timeout issues
- Estimated: Would perform well if time constraints removed
- Too slow for contest server requirements

LESSONS LEARNED:
- Two-layer planning (PDDL + Q-learning) is too heavy for real-time game
- Need simpler low-level execution
- PDDL overhead must be minimized

--------------------------------------------------------------------------------
ITERATION 2: Simplified Pure Offensive (myTeam_simple_backup.py)
--------------------------------------------------------------------------------
File: Assignment_2/backup_files/myTeam_simple_backup.py
Lines of Code: 175
Date: Early November 2025
Agent Type: OffensiveAgent

APPROACH:
- Complete removal of PDDL and Q-learning
- Pure heuristic-based greedy agent
- Both agents use identical offensive strategy
- Focus on simplicity and speed

KEY FEATURES:
✓ Lightweight implementation (only 175 lines)
✓ Aggressive food collection strategy
✓ Simple ghost avoidance logic
✓ Loop detection to prevent getting stuck
✓ Dynamic escape when carrying food
✓ No training required

STRATEGY:
1. Primary Goal: Collect food aggressively
2. Safety Check: If carrying food AND ghost nearby (≤5 distance) → Escape
3. Return Threshold: Return home when carrying 5+ food
4. Loop Detection: Random action if stuck in 2-position loop

DECISION LOGIC:
```python
if carrying_food and ghost_distance <= 5:
    escape_home()
elif carrying >= 5:
    escape_home()
else:
    attack_food()
```

GHOST AVOIDANCE:
- Avoid positions within 1 step of ghost (hard constraint)
- Add penalty for positions near ghosts (distance-based)
- Only considers non-scared ghosts

PROBLEMS ENCOUNTERED:
✗ Too aggressive - didn't know when to return
✗ No defensive capability
✗ Both agents doing same thing (no coordination)
✗ Simple heuristics beaten by strategic opponents
✗ No capsule strategy
✗ No role differentiation

PERFORMANCE:
- Estimated win rate: 30-40% against staffTeam.py
- Fast execution, no timeout issues
- Predictable behavior made it easy to counter

LESSONS LEARNED:
- Pure greedy approach insufficient for competitive play
- Need some form of strategic planning
- Role differentiation important
- Defensive capability necessary

--------------------------------------------------------------------------------
ITERATION 3: Enhanced PDDL with Q-Learning (myTeam_simple.py)
--------------------------------------------------------------------------------
File: Assignment_2/backup_files/myTeam_simple.py
Lines of Code: 1,299
Date: Mid November 2025
Agent Type: MixedAgent (Enhanced)

APPROACH:
- Return to PDDL + Q-learning but with improvements
- Enhanced weight sets with more features
- Better agent coordination through class variables
- Improved low-level planners

KEY FEATURES:
✓ Enhanced Q-learning with 5 weight sets (vs 3 in Iteration 1)
✓ More sophisticated features in weight calculations
✓ Team coordination through TEAM_STRATEGY class variable
✓ Global INVADER_ALERT system
✓ Action history tracking to prevent loops
✓ Improved PDDL state representation

ENHANCED WEIGHT SETS:
1. offensiveWeights (11 features):
   - closest-food: -10 (increased from -3)
   - #-of-ghosts-1-step-away: -200 (increased from -100)
   - chance-return-food: 50 (increased from 10)
   - NEW: closest-capsule: -8
   - NEW: can-eat-scared-ghost: 300
   - NEW: stop: -300 (heavy penalty)
   - NEW: is-moving: 20 (encourage movement)

2. defensiveWeights (9 features):
   - numInvaders: -1500 (increased from -1000)
   - invaderDistance: -20 (increased from -10)
   - NEW: chase-invader: 100
   - NEW: protect-food: 30
   - NEW: is-moving: 20

3. escapeWeights (7 features):
   - onDefense: 1500 (increased from 1000)
   - distanceToHome: -30 (increased from -20)
   - NEW: carrying-food: 40
   - NEW: ghost-very-close: -800
   - NEW: is-moving: 30

4. huntScaredWeights (NEW - 6 features):
   - scared-ghost-distance: -80
   - can-eat-ghost: 800
   - successorScore: 300
   - stop: -400
   - is-moving: 40

5. capsuleHuntWeights (NEW - 6 features):
   - closest-capsule: -15
   - capsule-value: 150
   - ghost-nearby: -30
   - stop: -300
   - is-moving: 20

COORDINATION FEATURES:
- ACTION_HISTORY: Track repeated actions to prevent loops
- TEAM_STRATEGY: Share role assignments between agents
- INVADER_ALERT: Global flag for defensive coordination

IMPROVEMENTS OVER ITERATION 1:
✓ 2 additional low-level planners (huntScared, capsuleHunt)
✓ 40+ new features across all weight sets
✓ Better movement encouragement (is-moving feature)
✓ Stronger ghost avoidance penalties
✓ Capsule strategy integration
✓ Team coordination variables

PROBLEMS ENCOUNTERED:
✗ Still too slow - timeout issues persisted
✗ 1,299 lines hard to debug and maintain
✗ Q-learning features required extensive tuning
✗ Training still needed for optimal performance
✗ PDDL + Q-learning interaction still complex

PERFORMANCE:
- Not fully tested due to timeout issues
- Potentially strong if time constraints removed
- Too complex for reliable real-time execution

LESSONS LEARNED:
- Adding more features doesn't solve fundamental speed problem
- Need to choose: either PDDL OR Q-learning, not both
- Simpler is better for time-constrained games
- Code complexity makes debugging difficult

--------------------------------------------------------------------------------
ITERATION 4: PDDL-Only Hybrid (myTeam_new.py)
--------------------------------------------------------------------------------
File: Assignment_2/backup_files/myTeam_new.py
Lines of Code: 402
Date: Mid-Late November 2025
Agent Type: HybridAgent

APPROACH:
- PDDL for strategic planning ONLY
- Replace Q-learning with simple greedy heuristics
- Focus on fast execution
- Simplified architecture

KEY FEATURES:
✓ PDDL solver for high-level action selection
✓ Simple greedy heuristics for low-level movement
✓ No training required
✓ Loop detection
✓ Periodic replanning (every 10 steps)
✓ Fast execution

ARCHITECTURE CHANGES:
Before (Iteration 3): PDDL → Q-Learning → Action
After (Iteration 4): PDDL → Greedy Heuristic → Action

REPLANNING TRIGGERS:
1. No current plan exists
2. 10 steps executed since last plan
3. Situation changed drastically (e.g., ghost suddenly close)

HIGH-LEVEL ACTIONS (from PDDL):
- attack / attack_aggressive → collectFood()
- return_food / go_home → returnHome()
- hunt_invader / defence → defendHome()
- get_capsule → getCapsule()
- hunt_scared → huntGhost()
- patrol → patrol()

LOW-LEVEL IMPLEMENTATIONS:
All use simple greedy maze distance calculations:
1. collectFood(): Chase closest food, avoid ghosts
2. returnHome(): Move toward home column, avoid ghosts
3. defendHome(): Chase closest invader
4. getCapsule(): Move toward capsule
5. huntGhost(): Chase scared ghosts
6. patrol(): Position at defensive center

GHOST AVOIDANCE:
- Skip actions that put agent within 1 step of ghost
- Add distance-based penalty (3-step range)
- Only avoid non-scared ghosts

SIMPLIFICATIONS FROM ITERATION 3:
✗ Removed: Q-learning entirely (saved ~600 lines)
✗ Removed: Training mode
✗ Removed: Weight persistence
✗ Removed: All 5 weight sets
✗ Removed: Complex feature calculations
✓ Kept: PDDL strategic planning
✓ Kept: Loop detection
✓ Added: Simple greedy heuristics

ADVANTAGES:
✓ Much faster execution (no Q-learning overhead)
✓ No training required
✓ Easier to debug (402 vs 1,299 lines)
✓ More predictable behavior
✓ Passes time constraints

DISADVANTAGES:
✗ Greedy heuristics not optimal
✗ No learning from experience
✗ Limited tactical sophistication
✗ Basic ghost avoidance only

PERFORMANCE:
- Estimated: 40-50% win rate
- Fast enough for contest server
- Reliable execution without timeouts

LESSONS LEARNED:
- Simplicity enables reliability
- PDDL provides enough strategic guidance
- Greedy heuristics sufficient for low-level execution
- This is the right balance for the assignment

--------------------------------------------------------------------------------
ITERATION 5: FINAL SUBMISSION (myTeam.py - Current)
--------------------------------------------------------------------------------
File: pacman-public/myTeam.py (current production file)
Lines of Code: 513
Date: November 3, 2025 (submitted)
Agent Type: HybridAgent
Contest Status: NOMINATED (commit a24b2ec)
Performance: 10W 12L 0T (45.5% win rate, 1081 EP)

APPROACH:
- Based on Iteration 4 (myTeam_new.py) with refinements
- PDDL for strategic decisions + greedy heuristics for execution
- Production-ready, tested implementation

KEY REFINEMENTS FROM ITERATION 4:
✓ Better PDDL state representation (more predicates)
✓ Improved ghost avoidance logic
✓ Smarter return-home triggers (dynamic threshold)
✓ Enhanced food targeting (safety scoring)
✓ Better patrol positioning
✓ More robust error handling

PDDL STATE PREDICATES USED:
Basic state:
- food_available
- food_in_backpack (with quantity: 2+, 3+, 5+, 10+, 20+)
- is_pacman

Enemy state:
- enemy_around (≤4 distance)
- is_scared (enemy scared timer > 0)
- can_hunt_ghosts
- scared_ghost_near

Tactical state:
- capsule_available
- near_capsule (≤4 distance)
- near_food (≤4 distance)
- invaders_present

Role assignment:
- is_attacker (first agent)
- is_defender (second agent)

GOAL PRIORITIZATION (3-tier system):
Priority 1: Return home if carrying food (2+ threshold, 3+ if ghosts far)
Priority 2: Defend if invaders present and we're defender
Priority 3: Collect all enemy food (default offense)

DECISION FLOW:
```
gameState → shouldReplan? → updatePlan (PDDL) → currentAction
                                                      ↓
                                              executeAction (greedy)
                                                      ↓
                                                 choose move
```

LOW-LEVEL EXECUTION IMPROVEMENTS:
1. collectFood():
   - Safety scoring: penalty for food near ghosts
   - Emergency return: if ghost ≤2 and carrying food
   - Best food selection: closest + safest

2. returnHome():
   - Dynamic threshold: return at 2 food if threatened, 3 normally
   - Smart home targeting: find closest valid home position
   - Path safety: avoid ghost-blocked routes

3. defendHome():
   - Priority targeting: chase closest invader
   - Fallback to patrol if no visible invaders
   - Aggressive chase (no ghost avoidance)

4. patrol():
   - Calculate defensive center from food positions
   - Position on boundary near center
   - Spread out from ally (if implemented)

ACTUAL PERFORMANCE METRICS:
✓ Contest submission: Successful
✓ Time constraints: Passed
✓ Win rate: 45.5% (10W 12L)
✓ EP score: 1081
✓ Status: NOMINATED

ANALYSIS OF RESULTS:
Strengths:
+ Reliable execution (no crashes/timeouts)
+ Decent offensive capability
+ Basic defensive response
+ Fast replanning

Weaknesses:
- Below 57% target win rate
- Simple goal prioritization
- No dynamic strategy adjustment
- Limited agent coordination
- Greedy heuristics sometimes suboptimal

COMPARISON TO TARGET:
Target: 57% win rate (28/49 games)
Actual: 45.5% win rate (10/22 games)
Gap: -11.5 percentage points

GRADING IMPACT:
Criteria 1 (Baseline): ~40-50% (below pass threshold)
Criteria 2 (Strategy): CREDIT level achieved
- ✓ New PDDL actions implemented
- ✓ Basic goal functions
- ✗ No dynamic goal switching
- ✗ Low-level planner not significantly improved

Expected Grade: CREDIT (60-69%)

--------------------------------------------------------------------------------
COMPARISON SUMMARY
--------------------------------------------------------------------------------

Implementation Progression:

Iteration  | Lines | Approach        | Speed   | Win Rate Est. | Status
-----------|-------|-----------------|---------|---------------|----------
1. Old     | 708   | PDDL+Q-Learn    | Slow    | N/A           | Timeout
2. Simple  | 175   | Pure Greedy     | Fast    | 30-40%        | Too weak
3. Enhanced| 1299  | PDDL+Q-Learn++  | Slow    | N/A           | Timeout
4. New     | 402   | PDDL+Greedy     | Fast    | 40-50%        | Viable
5. Final   | 513   | PDDL+Greedy+    | Fast    | 45.5%         | SUBMITTED

Key Insight:
The progression shows that finding the right balance between sophistication
and speed was critical. Iterations 1 & 3 were too complex, iteration 2 was
too simple, and iterations 4 & 5 found the sweet spot.

Code Complexity vs Performance:
- More code ≠ better performance
- Optimal solution: ~400-500 lines
- PDDL alone sufficient for strategic planning
- Greedy heuristics adequate for low-level execution

What Worked:
✓ PDDL for high-level strategy
✓ Simple greedy heuristics for movement
✓ Loop detection
✓ Periodic replanning
✓ Role assignment (attacker/defender)

What Didn't Work:
✗ Q-learning (too slow)
✗ Training-based approaches (time constraint)
✗ Complex feature engineering
✗ Multi-layer planning

================================================================================
ACTUAL IMPLEMENTATION DETAILS - FINAL VERSION
================================================================================

------------------------------------------------------------------------
OFFENSIVE ACTIONS (3 new actions)
------------------------------------------------------------------------

1. ACTION: aggressive_attack
   Parameters: current_agent, enemy1, enemy2
   Precondition: Agent is pacman, food available, both enemies are scared
   Effect: Food collected (not food_available)
   
   Reasoning:
   - Capitalizes on post-capsule advantage
   - Enemies are scared, so aggressive collection is safe
   - Maximizes food collection during power-up window
   - PDDL will choose this over regular attack when enemies scared

2. ACTION: cautious_attack
   Parameters: current_agent, enemy1, enemy2
   Precondition: Agent at home, food available, enemies far away
   Effect: Agent becomes pacman (enters enemy territory)
   
   Reasoning:
   - Initiates offense only when enemies are distant (>25 units)
   - Reduces risk of immediate confrontation
   - Safe entry into enemy territory
   - Different from regular attack which assumes already in enemy land

------------------------------------------------------------------------
RETURN HOME ACTIONS (4 new strategic returns)
------------------------------------------------------------------------

3. ACTION: emergency_return
   Parameters: current_agent, enemy
   Precondition: Agent is pacman, has food, enemy nearby, enemy not scared
   Effect: Return home (not is_pacman), food secured
   
   Reasoning:
   - Highest priority return when in immediate danger
   - Prevents losing collected food to enemy ghosts
   - Triggered by enemy_around predicate (within 4 grid distance)
   - Critical for preserving score progress

4. ACTION: safe_return
   Parameters: current_agent, enemy1, enemy2
   Precondition: Agent is pacman, 5+ food, both enemies far (>25 units)
   Effect: Return home, food secured
   
   Reasoning:
   - Optimal return timing with good food count
   - Minimal risk since enemies are far away
   - Balances greed (more food) vs safety (secure points)
   - Uses 5_food_in_backpack and enemy_long_distance predicates

5. ACTION: greedy_return
   Parameters: current_agent
   Precondition: Agent is pacman, 20+ food in backpack
   Effect: Return home, food secured
   
   Reasoning:
   - Unconditional return with massive food count
   - 20 food is excellent haul, don't risk losing it
   - No enemy check needed - priority is securing the score
   - Prevents catastrophic loss from being caught

6. ACTION: medium_return
   Parameters: current_agent, enemy1, enemy2
   Precondition: Agent is pacman, 10+ food, enemies medium distance (>15)
   Effect: Return home, food secured
   
   Reasoning:
   - Balanced return with good food count
   - Enemies not far but not close - acceptable risk to return
   - 10 food is significant, worth securing
   - Uses enemy_medium_distance predicate

Strategy Impact:
- PDDL solver will choose appropriate return based on context
- Replaces simple "go_home" with intelligent decision-making
- Each action has different risk/reward profile
- Collectively enables adaptive behavior

------------------------------------------------------------------------
CAPSULE TACTICAL ACTIONS (2 new actions)
------------------------------------------------------------------------

7. ACTION: eat_capsule_tactical
   Parameters: current_agent, enemy
   Precondition: Agent is pacman, near capsule, capsule exists, enemy nearby
   Effect: Capsule consumed, enemy becomes scared
   
   Reasoning:
   - Uses capsule defensively when threatened
   - Turns danger (nearby enemy) into opportunity (scared enemy)
   - Enables escape or aggressive collection
   - Tactical use rather than random capsule consumption

8. ACTION: eat_capsule_preemptive
   Parameters: current_agent, enemy
   Precondition: Agent is pacman, near capsule, enemy at short distance
   Effect: Capsule consumed, enemy becomes scared
   
   Reasoning:
   - Proactive capsule use before enemy gets too close
   - Creates safe zone for food collection
   - Uses enemy_short_distance predicate (<15 units)
   - Preventative rather than reactive

Strategy Impact:
- Capsules become strategic tools, not opportunistic pickups
- PDDL plans capsule usage as part of attack strategy
- Maximizes value of limited capsule resources

------------------------------------------------------------------------
DEFENSIVE ACTIONS (5 new actions)
------------------------------------------------------------------------

9. ACTION: aggressive_defense
   Parameters: current_agent, enemy
   Precondition: Agent at home, enemy is pacman nearby, not winning
   Effect: Enemy eliminated (not is_pacman)
   
   Reasoning:
   - Prioritizes defense when losing
   - Can't afford to let enemies score
   - Chases invaders actively
   - Complements offensive strategy when behind

10. ACTION: priority_defense
    Parameters: current_agent, enemy
    Precondition: Agent at home, enemy is pacman, enemy has 5+ food
    Effect: Enemy eliminated
    
    Reasoning:
    - Targets high-value threats
    - Enemy with 5+ food could swing the game
    - Higher priority than enemies with little food
    - Intelligent target selection

11. ACTION: emergency_defense
    Parameters: current_agent, enemy
    Precondition: Agent is pacman, enemy is pacman with 10+ food
    Effect: Both return home (agent and enemy eliminated as pacman)
    
    Reasoning:
    - Extreme measure: abandon own offense to stop major threat
    - Enemy has 10+ food = potential 10+ point swing
    - Sacrifice own position to save the game
    - Demonstrates sophisticated priority reasoning

12. ACTION: zone_defense
    Parameters: current_agent, enemy1, enemy2
    Precondition: Agent at home, no enemies invading, winning by 3+, 
                  not near ally
    Effect: Defend foods (patrol)
    
    Reasoning:
    - Spread out defensive coverage
    - Don't cluster with ally (not near_ally predicate)
    - Efficient territory coverage
    - Maintains defensive positioning when ahead

13. ACTION: patrol (enhanced)
    Parameters: current_agent, enemy1, enemy2
    Precondition: Agent at home, no enemies invading, winning by 10+
    Effect: Defend foods
    
    Reasoning:
    - Conservative play when far ahead
    - Don't take risks with large lead
    - Prevent any enemy incursions
    - Run out the clock

Strategy Impact:
- Multi-tiered defense based on game state
- Intelligent threat assessment (food carried by enemy)
- Dynamic role switching (offense to defense)
- Coordinate with ally through positioning

================================================================================
PART 2: PYTHON GOAL FUNCTION ENHANCEMENTS (myTeam.py)
================================================================================

CHANGE: Completely rewrote getGoals() function
--------------------------------------------------
Old Implementation:
- Simple 2-way decision: winning_gt10 or not
- Only 2 goal functions: goalDefWinning and goalScoring
- No consideration of immediate threats
- No food security logic

New Implementation:
- 6-tier priority system
- 6 different goal functions for different situations
- Context-aware decision making
- Adaptive strategy based on game state

------------------------------------------------------------------------
PRIORITY SYSTEM EXPLANATION
------------------------------------------------------------------------

PRIORITY 1: Emergency Defense (goalEmergencyDefense)
Trigger: Enemy has 10+ food on our territory
Goal: Eliminate that specific enemy (negative goal: not is_pacman for that enemy)

Reasoning:
- Overrides ALL other goals
- 10+ food loss could lose the game
- Both agents should focus on this threat
- Immediate response required

Implementation Details:
- Scans all enemy objects
- Checks for is_pacman AND 10_food_in_backpack predicates
- Returns specific enemy as negative goal
- PDDL will plan emergency_defense or priority_defense action

PRIORITY 2: Secure Food (goalSecureFood)
Trigger: Current agent has 20+ food and is in enemy territory
Goal: Food secured (positive goal: food_secured)

Reasoning:
- 20 food is massive score potential
- Must return home immediately
- Overrides normal offense
- Prevents catastrophic loss

Implementation Details:
- Checks own food count (20_food_in_backpack)
- Checks if currently pacman (in enemy territory)
- Sets food_secured as positive goal
- PDDL will choose greedy_return action

PRIORITY 3: Defensive Winning (goalDefWinning)
Trigger: Winning by 10+ points
Goal: Defend foods (positive goal: defend_foods)
      + Eliminate all invaders (negative goals)

Reasoning:
- Large lead = conservative play
- Don't risk losing lead with aggressive offense
- Focus on preventing enemy scores
- Run clock and protect territory

Implementation Details:
- defend_foods as positive goal triggers patrol/zone_defense
- Also adds negative goals for any enemy pacman
- Ensures dual focus: patrol AND eliminate invaders

PRIORITY 4: Balanced Play (goalBalanced)
Trigger: Winning by 3-10 points
Goal: Context-dependent (food_secured if carrying food, else continue scoring)

Reasoning:
- Moderate lead needs protection but can't be too passive
- If carrying food, secure it (preserve lead)
- If not carrying food, continue offense (extend lead)
- Dynamic goal based on current state

Implementation Details:
- Checks if agent has food in backpack
- If yes: goal = food_secured (return home)
- If no: goal = eliminate food_available (continue offense)
- Also defends against ALL invaders (more defensive than aggressive mode)

PRIORITY 5: Aggressive Offense (goalAggressiveScoring)
Trigger: Losing (score <= 0)
Goal: Eliminate food_available (collect all food)

Reasoning:
- Behind in score = need points urgently
- Accept more risk
- Less defensive attention
- Only defend against enemies with 5+ food

Implementation Details:
- Primary goal: food_available = false (collect all food)
- Reduced defense: only stop enemies with 5+ food
- Allows small enemy incursions to focus on offense
- Aggressive risk/reward calculation

PRIORITY 6: Standard Scoring (goalScoring)
Trigger: Default / slight lead (1-2 points)
Goal: Eliminate food_available + defend territory

Reasoning:
- Balanced approach
- Collect food while defending
- Standard competitive play
- No special circumstances

Implementation Details:
- Negative goal: food_available (offensive)
- Negative goals: all enemy pacman (defensive)
- Balanced priorities

------------------------------------------------------------------------
NEW GOAL FUNCTIONS ADDED
------------------------------------------------------------------------

Function: goalEmergencyDefense(objects, initState, enemy_obj)
Purpose: Stop specific high-threat enemy
Returns: ([], [(not is_pacman, enemy_obj)])
Used by: Priority 1

Function: goalSecureFood(objects, initState)
Purpose: Return home with large food haul
Returns: ([(food_secured,)], [])
Used by: Priority 2

Function: goalAggressiveScoring(objects, initState)
Purpose: Desperate offense when losing
Returns: ([], [(food_available,)] + selective enemy defense)
Used by: Priority 5

Function: goalBalanced(objects, initState)
Purpose: Mixed offense/defense when moderately ahead
Returns: Context-dependent based on current agent state
Used by: Priority 4

Function: goalDefWinning(objects, initState) - ENHANCED
Purpose: Conservative defense when far ahead
Returns: ([(defend_foods,)], [(not is_pacman, all_enemies)])
Used by: Priority 3
Changes: Added explicit enemy elimination goals

Function: goalScoring(objects, initState) - UNCHANGED
Purpose: Standard balanced play
Returns: ([], [(food_available,)] + all enemy defense)
Used by: Priority 6

================================================================================
STRATEGIC ADVANTAGES OF NEW IMPLEMENTATION
================================================================================

1. CONTEXT-AWARE PLANNING
   - Actions adapt to score differential
   - Different strategies for winning vs losing
   - Food amount influences decisions

2. RISK MANAGEMENT
   - Multiple return strategies based on danger level
   - Emergency actions for critical situations
   - Calculated risk-taking when behind

3. RESOURCE OPTIMIZATION
   - Capsules used tactically, not randomly
   - Food collection vs security trade-offs
   - Defensive effort allocated by threat level

4. DYNAMIC ROLE ASSIGNMENT
   - Agents can switch offense/defense
   - Coordinated through PDDL goals
   - Responds to teammate needs

5. PRIORITY-BASED REASONING
   - Critical threats override normal goals
   - Hierarchical decision structure
   - Prevents tunnel vision on single strategy

================================================================================
EXPECTED PERFORMANCE IMPROVEMENTS
================================================================================

Against staffTeam.py Baseline:
- Better food security (fewer losses to enemy ghosts)
- Smarter return timing (optimal risk/reward)
- Improved defense (threat prioritization)
- Adaptive strategy (responds to score)

Predicted Win Rate: 60-70% (28-35 wins out of 49 games)
- Passes Criteria 1 (>28 wins required)
- Positions for Distinction level on strategy (Criteria 2)

Against Competition:
- Sophisticated planning beats simple heuristics
- Context awareness handles diverse opponents
- Emergency responses prevent catastrophic losses
- Balanced play maintains competitive advantage

================================================================================
ALIGNMENT WITH MARKING CRITERIA
================================================================================

Criteria 2: Agent Strategy (50 marks)

Target: DISTINCTION (70-79%) - "Dynamic goal switching + improved low-level planner"

Evidence of Dynamic Goal Switching:
✓ 6 different goal functions
✓ Priority-based selection system
✓ Context-aware decision making
✓ Score-dependent strategy changes
✓ Threat-responsive planning

PDDL Improvements:
✓ 13 total actions (was 4, added 9)
✓ Uses advanced predicates (enemy distances, food counts, scared timers)
✓ Multiple actions per category (offense, defense, return)
✓ Sophisticated preconditions

This implementation demonstrates:
- NEW PDDL actions using MORE predicates (meets C level)
- New goal functions (meets C level)
- Dynamic goal switching (meets D level)
- Customized strategies per situation (approaches HD level)

Note: To reach full HD (80-100%), need to also implement:
- Customized low-level plans per high-level action (Q-learning improvements)
- Agent cooperation/information sharing (next phase)

================================================================================
NEXT STEPS FOR FURTHER IMPROVEMENT
================================================================================

1. LOW-LEVEL PLANNING ENHANCEMENTS
   - Improve Q-learning features for each action type
   - Implement action-specific reward functions
   - Train on multiple maps

2. AGENT COORDINATION
   - Share information via class variables
   - Track ally current action in PDDL
   - Coordinate offensive/defensive roles
   - Avoid targeting same food

3. TIME-BASED STRATEGY
   - Track remaining game time
   - Desperate measures in final moves
   - Time-wasting when ahead near end

4. MAP ADAPTATION
   - Analyze map layout on initialization
   - Adjust distance thresholds for map size
   - Identify chokepoints and key positions

================================================================================
TESTING RECOMMENDATIONS
================================================================================

1. Test against staffTeam.py:
   cd /Users/mohitpandya/Monash/FIT5222_PAR/pacman-public
   python capture.py -r myTeam.py -b staffTeam.py -n 10 -q

2. Test on multiple maps:
   for i in {1..7}; do
     python capture.py -r myTeam.py -b staffTeam.py -n 7 -l RANDOM$i -q
   done

3. Check time constraints:
   python capture.py -r myTeam.py -b staffTeam.py -c

4. Debug PDDL planning:
   python capture.py -r myTeam.py -b staffTeam.py
   (watch console for PDDL action selection)

================================================================================
IMPLEMENTATION NOTES
================================================================================

Files Modified:
1. myTeam.pddl - Complete rewrite with 9 new actions
2. myTeam.py - Enhanced getGoals() and added 4 new goal functions

Files NOT Modified (yet):
- Low-level planning (Q-learning) - still uses baseline
- Feature extraction functions - still uses baseline
- Reward functions - still uses baseline

Backward Compatibility:
✓ All original actions still present
✓ goalScoring and goalDefWinning still available
✓ Existing low-level planning unchanged
✓ Can run immediately without breaking

Configuration:
- Training disabled (self.training should be False for submission)
- Distance thresholds: CLOSE=4, MEDIUM=15, LONG=25
- PDDL solver path: configured in registerInitialState()

================================================================================
END OF IMPLEMENTATION LOG
================================================================================
